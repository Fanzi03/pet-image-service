from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from diffusers import StableDiffusionPipeline
import base64
from io import BytesIO
import logging
from typing import Optional
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Stable Diffusion API", version="1.0.0")

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞
pipeline: Optional[StableDiffusionPipeline] = None

class GenerateRequest(BaseModel):
    prompt: str
    width: int = 512
    height: int = 512
    num_inference_steps: int = 20
    guidance_scale: float = 7.5

@app.on_event("startup")
async def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
    global pipeline
    try:
        logger.info("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º Stable Diffusion –º–æ–¥–µ–ª—å...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if torch.cuda.is_available():
            device = "cuda"
            torch_dtype = torch.float16
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º CUDA")
        else:
            device = "cpu"
            torch_dtype = torch.float32
            logger.info("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU (–±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ)")
        
        # –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        model_id = os.getenv("HF_MODEL_REPO", "runwayml/stable-diffusion-v1-5")
        model_path = os.getenv("MODEL_PATH", None)
        
        logger.info(f"üìÇ –ú–æ–¥–µ–ª—å: {model_path if model_path else model_id}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        if model_path and os.path.exists(model_path):
            # –ï—Å–ª–∏ –µ—Å—Ç—å –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
            pipeline = StableDiffusionPipeline.from_pretrained(
                model_path,
                torch_dtype=torch_dtype,
                use_safetensors=True,
                local_files_only=True
            )
            logger.info("üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å")
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ HuggingFace
            pipeline = StableDiffusionPipeline.from_pretrained(
                model_id,
                torch_dtype=torch_dtype,
                use_safetensors=True
            )
            logger.info("üåê –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –∏–∑ HuggingFace")
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        pipeline = pipeline.to(device)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        if device == "cuda":
            pipeline.enable_attention_slicing()
            try:
                pipeline.enable_xformers_memory_efficient_attention()
                logger.info("‚ú® –í–∫–ª—é—á–µ–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è xformers")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è xformers –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        else:
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è CPU
            pipeline.enable_attention_slicing()
        
        logger.info("üéâ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
        
        # –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞
        logger.info("üî• –ü—Ä–æ–≥—Ä–µ–≤–∞–µ–º –º–æ–¥–µ–ª—å...")
        with torch.no_grad():
            _ = pipeline("test", num_inference_steps=1, guidance_scale=1.0, width=64, height=64)
        logger.info("‚úÖ –ú–æ–¥–µ–ª—å –ø—Ä–æ–≥—Ä–µ—Ç–∞!")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        raise e

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="–ú–æ–¥–µ–ª—å –µ—â–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è")
    
    return {
        "status": "healthy",
        "model_loaded": True,
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "model": os.getenv("HF_MODEL_REPO", "runwayml/stable-diffusion-v1-5"),
        "torch_version": torch.__version__
    }

@app.post("/generate")
async def generate_image(request: GenerateRequest):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è - –æ—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è Java –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    try:
        logger.info(f"üé® –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º: '{request.prompt}' ({request.width}x{request.height})")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if request.width > 1024 or request.height > 1024:
            raise HTTPException(status_code=400, detail="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ 1024x1024")
        
        if request.num_inference_steps > 50:
            raise HTTPException(status_code=400, detail="–ú–∞–∫—Å–∏–º—É–º 50 —à–∞–≥–æ–≤")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        device = next(pipeline.unet.parameters()).device
        
        with torch.autocast(device.type if device.type != "cpu" else "cpu"):
            result = pipeline(
                prompt=request.prompt,
                width=request.width,
                height=request.height,
                num_inference_steps=request.num_inference_steps,
                guidance_scale=request.guidance_scale,
                generator=torch.Generator(device=device).manual_seed(42)
            )
        
        image = result.images[0]
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64
        buffer = BytesIO()
        image.save(buffer, format="PNG", optimize=True)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        logger.info(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ! –†–∞–∑–º–µ—Ä: {len(image_base64)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return {
            "image": image_base64,
            "prompt": request.prompt,
            "width": request.width,
            "height": request.height,
            "steps": request.num_inference_steps,
            "guidance": request.guidance_scale
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")

@app.get("/")
async def root():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± API"""
    return {
        "service": "Stable Diffusion API",
        "version": "1.0.0",
        "model": os.getenv("HF_MODEL_REPO", "runwayml/stable-diffusion-v1-5"),
        "endpoints": {
            "health": "GET /health - –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏",
            "generate": "POST /generate - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
            "docs": "GET /docs - Swagger –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
        },
        "status": "ready" if pipeline else "loading"
    }

@app.get("/status")
async def get_status():
    """–î–µ—Ç–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "model_loaded": pipeline is not None,
        "cuda_available": torch.cuda.is_available(),
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "torch_version": torch.__version__,
        "model_path": os.getenv("MODEL_PATH", "HuggingFace"),
        "model_repo": os.getenv("HF_MODEL_REPO", "runwayml/stable-diffusion-v1-5")
    }

if __name__ == "__main__":
    import uvicorn
    logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º Stable Diffusion API...")
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
